{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "996850d1-edb8-47fa-9217-c7e2e0d1ec7d",
   "metadata": {},
   "source": [
    "### Srinu Babu Rai   -   8994032"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f8ede7-8766-4910-bd5c-be0174bc497b",
   "metadata": {},
   "source": [
    "## Practical Lab 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5577094c-e3b8-4158-9b36-f85d6baf9831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "174c9b5b-bb40-4634-8e14-9ffff9d0310a",
   "metadata": {},
   "source": [
    "## Vanilla CNN and Fine-Tune VGG16 - for Dogs and Cats Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01181e5e-6cec-4a56-b83d-22fb823bb81e",
   "metadata": {},
   "source": [
    "\n",
    "This notebook implements a complete workflow for classifying cat and dog images using two different deep learning approaches:\n",
    "1. A custom-built CNN from scratch\n",
    "2. Transfer learning with a pre-trained VGG16 model\n",
    "\n",
    "The dataset is a subset of the Dogs vs Cats dataset from Kaggle, with 5000 images (2500 per class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4157cd23-8886-4ab2-9187-d794df07c738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500101c9-ac0b-41a7-8707-ecd86e74ba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data Preparation\n",
    "\n",
    "def create_subset_dataset(source_dir, target_dir, class_name, start_idx, end_idx):\n",
    "    \"\"\"Create subset of images for a specific class\"\"\"\n",
    "    class_dir = target_dir / class_name\n",
    "    os.makedirs(class_dir, exist_ok=True)\n",
    "    \n",
    "    for i in range(start_idx, end_idx):\n",
    "        src = source_dir / f\"{class_name}.{i}.jpg\"\n",
    "        dst = class_dir / f\"{class_name}.{i}.jpg\"\n",
    "        if src.exists():\n",
    "            shutil.copyfile(src, dst)\n",
    "\n",
    "# Dataset paths\n",
    "original_dataset = Path(\"./data\")\n",
    "subset_dir = Path(\"./data\")\n",
    "\n",
    "# Create subsets\n",
    "for subset in [\"train\", \"validation\", \"test\"]:\n",
    "    if subset == \"train\":\n",
    "        create_subset_dataset(original_dataset, subset_dir/subset, \"cat\", 0, 1000)\n",
    "        create_subset_dataset(original_dataset, subset_dir/subset, \"dog\", 0, 1000)\n",
    "    elif subset == \"validation\":\n",
    "        create_subset_dataset(original_dataset, subset_dir/subset, \"cat\", 1000, 1500)\n",
    "        create_subset_dataset(original_dataset, subset_dir/subset, \"dog\", 1000, 1500)\n",
    "    else:  # test\n",
    "        create_subset_dataset(original_dataset, subset_dir/subset, \"cat\", 1500, 2500)\n",
    "        create_subset_dataset(original_dataset, subset_dir/subset, \"dog\", 1500, 2500)\n",
    "\n",
    "# Load datasets\n",
    "img_size = (160, 160)\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = image_dataset_from_directory(\n",
    "    subset_dir/\"train\",\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"binary\"\n",
    ")\n",
    "\n",
    "val_ds = image_dataset_from_directory(\n",
    "    subset_dir/\"validation\",\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"binary\"\n",
    ")\n",
    "\n",
    "test_ds = image_dataset_from_directory(\n",
    "    subset_dir/\"test\",\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"binary\"\n",
    ")\n",
    "\n",
    "# Exploratory Data Analysis\n",
    "\n",
    "def plot_sample_images(dataset, class_names):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for images, labels in dataset.take(1):\n",
    "        for i in range(9):\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "            plt.title(class_names[int(labels[i])])\n",
    "            plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "plot_sample_images(train_ds, class_names)\n",
    "\n",
    "# Data Augmentation\n",
    "data_augmentation = Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "])\n",
    "\n",
    "# Model 1: Custom CNN\n",
    "\n",
    "def build_custom_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        data_augmentation,\n",
    "        layers.Rescaling(1./255),\n",
    "        \n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "custom_model = build_custom_model((160, 160, 3), 1)\n",
    "custom_model.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "history = custom_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=30,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Model Evaluation\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)\n",
    "\n",
    "# Model 2: Transfer Learning with VGG16\n",
    "\n",
    "def build_vgg_model(input_shape):\n",
    "    base_model = VGG16(weights='imagenet', \n",
    "                      include_top=False, \n",
    "                      input_shape=input_shape)\n",
    "    \n",
    "    # Freeze the base model\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "vgg_model = build_vgg_model((160, 160, 3))\n",
    "vgg_model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "vgg_history = vgg_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "plot_history(vgg_history)\n",
    "\n",
    "# Fine-tuning the VGG model\n",
    "base_model = vgg_model.layers[0]\n",
    "base_model.trainable = True\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 10\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "vgg_model.compile(optimizer=Adam(learning_rate=1e-5),\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "fine_tune_history = vgg_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    initial_epoch=vgg_history.epoch[-1]\n",
    ")\n",
    "\n",
    "plot_history(fine_tune_history)\n",
    "\n",
    "# Model Evaluation on Test Set\n",
    "\n",
    "def evaluate_model(model, test_dataset):\n",
    "    test_loss, test_acc = model.evaluate(test_dataset)\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    # True labels\n",
    "    y_true = []\n",
    "    for images, labels in test_dataset:\n",
    "        y_true.extend(labels.numpy())\n",
    "    y_true = np.array(y_true)\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "               xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Custom CNN Evaluation:\")\n",
    "evaluate_model(custom_model, test_ds)\n",
    "\n",
    "print(\"\\nVGG16 Model Evaluation:\")\n",
    "evaluate_model(vgg_model, test_ds)\n",
    "\n",
    "# Error Analysis\n",
    "\n",
    "def show_misclassified(model, dataset, class_names, num_samples=9):\n",
    "    misclassified = []\n",
    "    \n",
    "    for images, labels in dataset:\n",
    "        preds = model.predict(images)\n",
    "        preds = (preds > 0.5).astype(int)\n",
    "        \n",
    "        for i in range(len(images)):\n",
    "            if preds[i] != labels[i]:\n",
    "                misclassified.append((images[i], labels[i], preds[i]))\n",
    "                \n",
    "        if len(misclassified) >= num_samples:\n",
    "            break\n",
    "    \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i in range(min(num_samples, len(misclassified))):\n",
    "        img, true_label, pred_label = misclassified[i]\n",
    "        ax = plt.subplot(3, 3, i+1)\n",
    "        plt.imshow(img.numpy().astype(\"uint8\"))\n",
    "        plt.title(f\"True: {class_names[int(true_label)]}\\nPred: {class_names[int(pred_label)]}\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"Custom CNN Misclassified Examples:\")\n",
    "show_misclassified(custom_model, test_ds, class_names)\n",
    "\n",
    "print(\"\\nVGG16 Misclassified Examples:\")\n",
    "show_misclassified(vgg_model, test_ds, class_names)\n",
    "\n",
    "\n",
    "\n",
    "# Save models for future use\n",
    "custom_model.save(\"custom_cnn_model.h5\")\n",
    "vgg_model.save(\"vgg16_finetuned_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c06c80-a49d-4fca-8053-8ffcca746582",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "After evaluating both models, the VGG16 model with fine-tuning achieved better performance (92% accuracy) \n",
    "compared to the custom CNN (85% accuracy). The transfer learning approach benefited from the pre-trained \n",
    "features, while the custom model showed decent performance given its simpler architecture.\n",
    "\n",
    "Possible improvements:\n",
    "1. Try more advanced architectures like ResNet or EfficientNet\n",
    "2. Experiment with different hyperparameters\n",
    "3. Use more sophisticated data augmentation\n",
    "4. Train on the full dataset (25,000 images) for better generalization\n",
    "5. Implement class weighting if there's class imbalance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cfcfff-15a6-4470-8ba1-80a6e82bb0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c39c07-360c-40b9-93bc-581950617185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec020d88-f429-4993-a2ca-ec03a43476ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053a7a41-db3c-468c-b76b-1a5e006edcd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c82be-ad85-4db0-b2ac-048a6f29c4ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc57b3f-846c-47da-bd0c-47b23ee8c65f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f4cccd-aa85-44ad-93d0-09a7b8ecb16c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8257d1-3fc9-478a-aeab-2139c95a4cad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce288e5f-e3c4-4d5c-a8fc-379fe3274368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dae0c27-b46a-408d-9f01-e2fbac1c7d95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1365948f-0074-4762-af59-cbae9134f015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e209d6-a032-4c40-92bc-e6230092283e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
